{
  "class": "MLP",
  "input_dim": 77,
  "layers": [
    128,
    64,
    1
  ],
  "activation": "ReLU",
  "dropout": 0.2
}